{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad62f82-96c1-4e4e-9bd5-fb305ddc9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For TAs: Set TRUE to run with mini sample \n",
    "\"\"\"\n",
    "MINI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cef8e0e-b6d3-49f4-bb41-c2d83e7d3a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 50, [7], [8, 9, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MINI:\n",
    "    train_days = [1]\n",
    "    test_days = [8]\n",
    "    EPOCHS = 3\n",
    "else:\n",
    "    train_days = [7]\n",
    "    test_days = [8,9,10] \n",
    "    EPOCHS = 50\n",
    "\n",
    "(MINI, EPOCHS, train_days, test_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1ae670-0920-48d8-99cf-e0cc21189f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.deep_lob import DeepLOB\n",
    "from models.trans_lob import TransLOB\n",
    "from models.deep_trans_lob import DeepTransLOB\n",
    "from utils.crypto_loader import load_crypto\n",
    "from utils.fi2010_loader import load_fi2010\n",
    "from utils.training import train_validate, train, validate\n",
    "from utils.plots import plot_training_history, plot_label_distributions\n",
    "from utils.preprocessing import generate_labels, normalize_features\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71ead22-ee97-4488-bf48-9d9afe26cbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8341cdea-3874-4f7d-8e57-7dfff8f789c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TYPE = \"fi2010\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "WINDOW_SIZE = 100\n",
    "LEVELS = 10\n",
    "HORIZONS=[10, 20, 30, 50, 100]\n",
    "TARGET_HORIZON = 20\n",
    "LABEL_ALPHA = 5e-5\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "ADAM_B1 = 0.9\n",
    "ADAM_B2 = 0.999\n",
    "WEIGHT_DECAY= 1e-5\n",
    "\n",
    "TEST_RATIO = 0.3\n",
    "\n",
    "T = 100\n",
    "stock = [0,1,2,3,4]\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5b936-b9de-4ae6-86e3-d545c104aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 254255\n",
      "Validation Data Size : 138102\n"
     ]
    }
   ],
   "source": [
    "for normalization in [\"DecPre\",\"Zscore\",\"MinMax\"]:\n",
    "    train_dataset, val_dataset = load_fi2010(TEST_RATIO, normalization, stock, \n",
    "                                                           train_days, test_days, T, k, True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Training Data Size : {train_dataset.__len__()}\")\n",
    "    print(f\"Validation Data Size : {val_dataset.__len__()}\")\n",
    "\n",
    "    model_deepLOB = DeepLOB().to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model_deepLOB.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    history = train_validate(model_deepLOB, train_loader, val_loader, optimizer, criterion, EPOCHS, normalization, DATASET_TYPE, device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f7db8-21b4-4784-914c-ffcabad64b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4d3f9e6-7144-4537-b451-ee9d18fffb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DeepLOB                                  [1, 3]                    --\n",
       "├─Sequential: 1-1                        [1, 32, 94, 20]           --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 100, 20]          96\n",
       "│    └─LeakyReLU: 2-2                    [1, 32, 100, 20]          --\n",
       "│    └─BatchNorm2d: 2-3                  [1, 32, 100, 20]          64\n",
       "│    └─Conv2d: 2-4                       [1, 32, 97, 20]           4,128\n",
       "│    └─LeakyReLU: 2-5                    [1, 32, 97, 20]           --\n",
       "│    └─BatchNorm2d: 2-6                  [1, 32, 97, 20]           64\n",
       "│    └─Conv2d: 2-7                       [1, 32, 94, 20]           4,128\n",
       "│    └─LeakyReLU: 2-8                    [1, 32, 94, 20]           --\n",
       "│    └─BatchNorm2d: 2-9                  [1, 32, 94, 20]           64\n",
       "├─Sequential: 1-2                        [1, 32, 88, 10]           --\n",
       "│    └─Conv2d: 2-10                      [1, 32, 94, 10]           2,080\n",
       "│    └─LeakyReLU: 2-11                   [1, 32, 94, 10]           --\n",
       "│    └─BatchNorm2d: 2-12                 [1, 32, 94, 10]           64\n",
       "│    └─Conv2d: 2-13                      [1, 32, 91, 10]           4,128\n",
       "│    └─LeakyReLU: 2-14                   [1, 32, 91, 10]           --\n",
       "│    └─BatchNorm2d: 2-15                 [1, 32, 91, 10]           64\n",
       "│    └─Conv2d: 2-16                      [1, 32, 88, 10]           4,128\n",
       "│    └─LeakyReLU: 2-17                   [1, 32, 88, 10]           --\n",
       "│    └─BatchNorm2d: 2-18                 [1, 32, 88, 10]           64\n",
       "├─Sequential: 1-3                        [1, 32, 82, 1]            --\n",
       "│    └─Conv2d: 2-19                      [1, 32, 88, 1]            10,272\n",
       "│    └─LeakyReLU: 2-20                   [1, 32, 88, 1]            --\n",
       "│    └─BatchNorm2d: 2-21                 [1, 32, 88, 1]            64\n",
       "│    └─Conv2d: 2-22                      [1, 32, 85, 1]            4,128\n",
       "│    └─LeakyReLU: 2-23                   [1, 32, 85, 1]            --\n",
       "│    └─BatchNorm2d: 2-24                 [1, 32, 85, 1]            64\n",
       "│    └─Conv2d: 2-25                      [1, 32, 82, 1]            4,128\n",
       "│    └─LeakyReLU: 2-26                   [1, 32, 82, 1]            --\n",
       "│    └─BatchNorm2d: 2-27                 [1, 32, 82, 1]            64\n",
       "├─Sequential: 1-4                        [1, 64, 82, 1]            --\n",
       "│    └─Conv2d: 2-28                      [1, 64, 82, 1]            2,112\n",
       "│    └─LeakyReLU: 2-29                   [1, 64, 82, 1]            --\n",
       "│    └─BatchNorm2d: 2-30                 [1, 64, 82, 1]            128\n",
       "│    └─Conv2d: 2-31                      [1, 64, 82, 1]            12,352\n",
       "│    └─LeakyReLU: 2-32                   [1, 64, 82, 1]            --\n",
       "│    └─BatchNorm2d: 2-33                 [1, 64, 82, 1]            128\n",
       "├─Sequential: 1-5                        [1, 64, 82, 1]            --\n",
       "│    └─Conv2d: 2-34                      [1, 64, 82, 1]            2,112\n",
       "│    └─LeakyReLU: 2-35                   [1, 64, 82, 1]            --\n",
       "│    └─BatchNorm2d: 2-36                 [1, 64, 82, 1]            128\n",
       "│    └─Conv2d: 2-37                      [1, 64, 82, 1]            20,544\n",
       "│    └─LeakyReLU: 2-38                   [1, 64, 82, 1]            --\n",
       "│    └─BatchNorm2d: 2-39                 [1, 64, 82, 1]            128\n",
       "├─Sequential: 1-6                        [1, 64, 82, 1]            --\n",
       "│    └─MaxPool2d: 2-40                   [1, 32, 82, 1]            --\n",
       "│    └─Conv2d: 2-41                      [1, 64, 82, 1]            2,112\n",
       "│    └─LeakyReLU: 2-42                   [1, 64, 82, 1]            --\n",
       "│    └─BatchNorm2d: 2-43                 [1, 64, 82, 1]            128\n",
       "├─LSTM: 1-7                              [1, 82, 64]               66,048\n",
       "├─Linear: 1-8                            [1, 3]                    195\n",
       "==========================================================================================\n",
       "Total params: 143,907\n",
       "Trainable params: 143,907\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 35.53\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 4.97\n",
       "Params size (MB): 0.58\n",
       "Estimated Total Size (MB): 5.56\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deepLOB = DeepLOB().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_deepLOB.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "summary(model_deepLOB, (1, 1, 100, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a802ec8-9c1d-4ed0-912f-8cd6249cb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 1.0338, Train Acc: 0.4797, Validation Loss: 1.0928, Validation Acc: 0.3935\n",
      "Epoch 2/3, Train Loss: 0.9580, Train Acc: 0.5773, Validation Loss: 1.0772, Validation Acc: 0.4290\n",
      "Epoch 3/3, Train Loss: 0.9048, Train Acc: 0.6399, Validation Loss: 1.0985, Validation Acc: 0.4160\n"
     ]
    }
   ],
   "source": [
    "### If MINI, do not save model \n",
    "\n",
    "if not MINI: \n",
    "    history = train_validate(model_deepLOB, train_loader, val_loader, optimizer, criterion, EPOCHS, NORMALIZATION, DATASET_TYPE, device)\n",
    "else:\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train(model_deepLOB, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate(model_deepLOB, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: \"\n",
    "              f\"Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | \"\n",
    "              f\"Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b21100-2d8f-487a-b04d-aa68c827c4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
